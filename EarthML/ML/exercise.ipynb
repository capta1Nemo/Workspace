{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","from matplotlib import pyplot as plt\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras import regularizers"]},{"cell_type":"markdown","metadata":{"id":"9GionNLWCvq1"},"source":["## EarthML TensorFlow Exercise Notebook 1: Linear Regression Workflow\n","\n","\n","Please submit this notebook after completion. You are encouraged to further experiment by yourself. Instructions in this notebook are just to guide you further on your learning."]},{"cell_type":"markdown","metadata":{"id":"6qgFDcaMCvq5"},"source":["---\n","\n","### Exercise 1: Linear Regression\n","\n","1.1. Generate a dataset with 100 data points for linear regression using the equation ($\n","y = x^3 - 5x^2 + 4x - 7 $ ) , where `x` is a random number between 1 and 10 (This is an arbitrary choice).\n","\n","Add some Gaussian noise to the output.\n","\n","1.2. Split the dataset into training and testing sets (80% train and 20% test).\n","\n","1.3. Implement a linear regression model in TensorFlow to predict `y` based on `x`.\n","\n","1.4. Train your model and visualize the loss reduction.\n","\n","1.5. Test your model on the test set and visualize the predicted line against the real data.\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xPBf_lgWCvq6"},"outputs":[],"source":["def gen_data(n,range_start,range_end,y_func):\n","    y=[]\n","    input=[]\n","    for i in range(n):\n","        x=np.random.uniform(range_start,range_end) \n","        input.append(x)\n","        y.append(y_func(x))\n","    return input, y    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def y_1(x):\n","    return x**3 - 5*x**2 + 4*x - 7\n","x_in=np.linspace(0,10,100)\n","y_in=y_1(x_in)\n","x,y=gen_data(100,1,10,y_1)\n","plt.plot(x,y,'o',color='blue')\n","plt.plot(x_in,y_in, color='red')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["noise=np.random.normal(scale=50,size=100)\n","y=y+noise\n","plt.plot(x,y,'o',color='blue')\n","plt.plot(x_in,y_in, color='red')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.DataFrame({\n","    'x': x,\n","    'y': y})\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train= df.sample(frac=0.8)\n","test= df.drop(train.index)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model=tf.keras.Sequential([tf.keras.layers.Dense(1,input_shape=(1,))])\n","model.summary()\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=1),\n","    loss=tf.keras.losses.MeanAbsoluteError())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["history=model.fit(train['x'],train['y'],epochs=200,verbose=1,validation_split=0.3)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hist=pd.DataFrame(history.history)\n","hist['epoch']=history.epoch\n","hist.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loss=model.evaluate(test['x'],test['y']) "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.axhline(loss,color='g',linestyle='--',label='test')\n","plt.plot(history.history['loss'],label='loss')\n","plt.plot(history.history['val_loss'],label='val_loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_predict=model.predict(test['x'])\n","plt.plot(test['x'],test['y'],'o',label='data')\n","plt.plot(test['x'],y_predict,'o',label='prediction')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["t=np.linspace(1,10,100).reshape(-1,1)\n","line=model.predict(t)\n","plt.plot(df['x'],df['y'],'o')\n","plt.plot(t,line)\n","plt.plot(x_in,y_in,linestyle='--')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"uBIatHIpCvq6"},"source":["---\n","### Exercise 2: Overfitting and Underfitting\n","\n","2.1. Generate a dataset with a quadratic relationship, for instance \\(y = x^2 + 2x + 3\\). Add Gaussian noise to the output.\n","\n","2.2. Split the dataset into training and testing sets.\n","\n","2.3. Train a simple linear model on the dataset and visualize the predictions.\n","\n","2.4. Now, implement a polynomial regression model of degree 2 and train it on the dataset.\n","\n","2.5. Compare the performance of the linear model and the polynomial model. Which one underfits? Which one is just right?\n","\n","2.6. Now, try a polynomial regression of degree 10. What do you observe regarding overfitting?\n","\n","---"]},{"cell_type":"markdown","metadata":{},"source":["2.6 da sorun yaşadım. model başta oluşturduğum datatyı normalize edip vermediğim sürece çok yüksek hatalar hesplıyor x in yüksek kuvvetlerinden ötürü. Farklı loss functıonları ve learning rateleri denedim hatta L2 regularızatıonu da bu kısımda denedim ama yalnızca normalize edilmiş data verdiğim zaman model düzgün çalışıyor. o yüzden overfit olmasını da sağlayamadım orijinal datayla bunu nasıl sağlayabilirim.   "]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JtonbpeLCvq7"},"outputs":[],"source":["def y_2(x):\n","    return 3*x**2- 4*x+ 7\n","x_in2=np.linspace(-10,10,100)\n","y_in2=y_2(x_in2)\n","x2,y2=gen_data(100,-10,10,y_2)\n","plt.plot(x2,y2,'o',color='blue')\n","plt.plot(x_in2,y_in2, color='red')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["noise=np.random.normal(scale=20,size=100)\n","y2=y2+noise\n","plt.plot(x2,y2,'o',color='blue')\n","plt.plot(x_in2,y_in2, color='red')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df2 = pd.DataFrame({\n","    'x': x2,\n","    'y': y2})\n","df2\n","train= df2.sample(frac=0.8)\n","test= df2.drop(train.index)\n","model2=tf.keras.Sequential([tf.keras.layers.Dense(1,input_shape=(1,))])\n","model2.summary()\n","model2.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=1),\n","    loss=tf.keras.losses.MeanAbsoluteError())\n","history2=model2.fit(train['x'],train['y'],epochs=200,verbose=0,validation_split=0.3)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["t=np.linspace(-10,10,100).reshape(-1,1)\n","line=model2.predict(t)\n","plt.plot(df2['x'],df2['y'],'o')\n","plt.plot(t,line)\n","plt.plot(x_in2,y_in2,linestyle='--')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["POLYNOMIAL REGRESSION"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x_sqr=[i**2 for i in x2]\n","df2_poly=pd.DataFrame({\n","    'x1': x2,\n","    'x2': x_sqr,\n","    'y' : y2})"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train= df2_poly.sample(frac=0.8)\n","test2= df2_poly.drop(train.index)\n","model2_poly=tf.keras.Sequential([tf.keras.layers.Dense(1,input_shape=(2,))])\n","model2_poly.summary()\n","model2_poly.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=0.4),\n","    loss=tf.keras.losses.MeanAbsoluteError())\n","poly_history=model2_poly.fit(train.drop(columns=['y']),train['y'],epochs=200,validation_split=0.3)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["t=[[x,x**2] for x in np.linspace(-10,10,100)]\n","line=model2_poly.predict(t)\n","plt.plot(df2_poly['x1'],df2_poly['y'],'o')\n","plt.plot([i[0] for i in t],line)\n","plt.plot(x_in2,y_in2,linestyle='--')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loss_linear=model2.evaluate(test['x'],test['y'])\n","loss_quadratic=model2_poly.evaluate(test2.drop(columns=['y']),test2['y'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(history2.history['loss'],label='loss')\n","plt.plot(history2.history['val_loss'],label='val_loss')\n","plt.title('linear')\n","plt.axhline(loss_linear,color='g',linestyle='--',label='test')\n","plt.legend()\n","plt.show()\n","plt.plot(poly_history.history['loss'],label='loss')\n","plt.plot(poly_history.history['val_loss'],label='val_loss')\n","plt.title('quadratic')\n","plt.axhline(loss_quadratic,color='g',linestyle='--',label='test')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["linear model underfits "]},{"cell_type":"markdown","metadata":{},"source":["2.6 da yaşadığım sorun burdan itibaren başlıyor."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def poly_datagen(n,x,y=None):\n","    df=pd.DataFrame({})\n","    for i in range(1,n+1):\n","        df[i]=[j**i for j in x]\n","    df['y']=y    \n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n=10\n","df3=poly_datagen(n,x2,y2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df3"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train= df3.sample(frac=0.8)\n","test= df3.drop(train.index)"]},{"cell_type":"markdown","metadata":{},"source":["bu modelde aynı 3. kısımda olduğu gibi normalizer vardı o şekilde düzgün sonuç alıyorum ama şuan olduğu haliyle neden çalışmadığını anlamadım.  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model3=tf.keras.Sequential([tf.keras.layers.Dense(1,input_shape=(n,))])\n","model3.summary()\n","model3.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=0.4),\n","    loss=tf.keras.losses.MeanSquaredError())\n","history3=model3.fit(train.drop(columns=['y']),train['y'],\n","                    epochs=500,verbose=1,validation_split=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(history3.history['loss'],label='loss')\n","plt.plot(history3.history['val_loss'],label='val_loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["t=[[i**j for j in range(1,11)] for i in np.linspace(-10,10,100)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["line=model3.predict(t)\n","plt.plot(df2_poly['x1'],df2_poly['y'],'o')\n","plt.plot([i[0] for i in t],line)\n","plt.ylim(-50,250)\n","plt.plot(x_in2,y_in2,linestyle='--')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["loss 2000 epoch sonra dahi buna benzer değğerlerde sabitleniyor daha fazla azaltmak için ne yapmak gerek. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model3.evaluate(test.drop(columns=['y']),test['y'])"]},{"cell_type":"markdown","metadata":{"id":"NPOEhw0YCvq7"},"source":["---\n","### Exercise 3: Regularization\n","\n","3.1. Continuing from the previous exercise, add L2 regularization to the polynomial regression model of degree 10.\n","\n","3.2. Train the model and compare its performance with the unregularized degree 10 model. What do you observe?\n","\n","3.3. Experiment with different regularization strengths. How does the strength of regularization affect the model?\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train= df3.sample(frac=0.8)\n","test= df3.drop(train.index)\n","L2=regularizers.L2\n","train_features=train.drop(columns=['y'])\n","normalizer = tf.keras.layers.Normalization()\n","normalizer.adapt(np.array(train_features))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7hYS02hYCvq8"},"outputs":[],"source":["modelL2=tf.keras.Sequential([tf.keras.layers.InputLayer(input_shape=(n,)),\n","                            normalizer,tf.keras.layers.Dense(1,\n","                            kernel_regularizer=L2(1e-2))])\n","modelL2.summary()\n","modelL2.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=0.5),\n","    loss=tf.keras.losses.MeanSquaredError())\n","history=modelL2.fit(train.drop(columns=['y']),train['y'],epochs=500,verbose=1,validation_split=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(history.history['loss'],label='loss')\n","plt.plot(history.history['val_loss'],label='val_loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["t=[[i**j for j in range(1,11)] for i in np.linspace(-15,15,100)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","line=modelL2.predict(t)\n","print(t[0])\n","print(line.shape)\n","plt.plot(df2_poly['x1'],df2_poly['y'],'o')\n","plt.plot([i[0] for i in t],line)\n","plt.ylim(-50,350)\n","plt.plot(x_in2,y_in2,linestyle='--')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["modelL2.evaluate(test.drop(columns=['y']),test['y'])"]},{"cell_type":"markdown","metadata":{"id":"JtfKKoHeCvq8"},"source":["---\n","### Exercise 4: Activation and Cost Functions\n","\n","4.1. Define and visualize the following activation functions: Sigmoid, ReLU, and Tanh. Describe their main properties and potential use cases.\n","\n","4.2. Implement a simple feed-forward neural network in TensorFlow for a binary classification problem. Use the sigmoid activation function for the output layer.\n","\n","4.3. Generate a toy binary classification dataset and train your neural network on it.\n","\n","4.4. Replace the sigmoid activation function in the hidden layers with ReLU and compare the performances. Which one trains faster?\n","\n","4.5. Experiment with different cost functions such as Mean Squared Error (MSE) and Cross-Entropy. What are their differences and which one is more suitable for the given problem?\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["inputs=np.linspace(-10,10,100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iup9LvWmCvq9"},"outputs":[],"source":["def reLU(x):\n","    return max(0.0, x)\n","\n","outputs = [reLU(x) for x in inputs]\n","plt.plot(inputs, outputs)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def sigmoid(x):\n"," return 1.0 / (1.0 + np.exp(-x))\n","\n","outputs = [sigmoid(x) for x in inputs]\n","plt.plot(inputs, outputs)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def tanh(x):\n"," return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n"," \n","outputs = [tanh(x) for x in inputs]\n","plt.plot(inputs, outputs)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["further explanation is in activation_function.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classification_model=tf.keras.Sequential(\n","    [tf.keras.layers.Dense(1,input_shape=(2,),activation='sigmoid')])\n","classification_model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n","    loss=tf.keras.losses.BinaryCrossentropy(),\n","    metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{},"source":["generate toy data with two features x1 and x2 and label y(0 or 1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X = np.random.uniform(-1, 1, (200,2))\n","print(X)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def generate_label2D(X):\n","    point1,point2 = np.random.uniform(-1, 1,(2,2))\n","    def classify(point):\n","        x1, y1 = point1\n","        x2, y2 = point2\n","        x, y = point\n","        #check whether the point is in the upside or downside region of target line \n","        cross_product = (x2 - x1) * (y - y1) - (y2 - y1) * (x - x1)\n","        if cross_product > 0:\n","            return 1\n","        else:\n","            return 0\n","    labels=[]    \n","    for i in X:\n","        labels.append(classify(i))\n","    return labels, (point1,point2)   \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["L=generate_label2D(X)\n","Y=L[0]\n","p1,p2=L[1]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df= pd.DataFrame({\n","    'x1': X[:,0],\n","    'x2': X[:,1],\n","    'label': Y\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df"]},{"cell_type":"markdown","metadata":{},"source":["visualizing the target function"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_line(p1,p2):\n","    m=(p2[1]-p1[1])/(p2[0]-p1[0])\n","    b=p1[1]-m*p1[0]\n","    return m,b"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["m,b= get_line(p1,p2)\n","t=np.linspace(-1,1,100)\n","plt.plot(t,m*t+b,linestyle='--')\n","plt.ylim(-1,1)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["showing the data with target function"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_map(X,Y,m,b,lable=None):\n","    for ind,x in enumerate(X):\n","        if Y[ind]==1:\n","            color = 'blue'\n","        else: color = 'red'\n","        plt.plot(x[0],x[1],'o', color=color) \n","    plt.plot(t,m*t+b,label=lable)\n","    if lable is not None: plt.legend()\n","    plt.ylim(-1,1)   "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_map(X,Y,m,b)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["history=classification_model.fit(df.drop(columns=['label']),df['label'],epochs=200,verbose=1,\n","                                 validation_split=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(history.history['loss'],label='loss')\n","plt.plot(history.history['val_loss'],label='val_loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classification_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classification_model.get_weights()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["coordinates=classification_model.get_weights()[0]\n","bias=classification_model.get_weights()[1]\n","\n","x0_1 = np.amin(X[:, 0])\n","x0_2 = np.amax(X[:, 0])\n","\n","x1_1 = (-coordinates[0] * x0_1 - bias) / coordinates[1]\n","x1_2 = (-coordinates[0] * x0_2 - bias) / coordinates[1]"]},{"cell_type":"markdown","metadata":{},"source":["visualization of data, target function and result of the model "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["m2,b2=get_line((x0_1,x1_1),(x0_2,x1_2))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_map(X,Y,m,b,'target')\n","get_map(X,Y,m2,b2,'output')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classification_model1=tf.keras.Sequential(\n","    [tf.keras.layers.Dense(1,input_shape=(2,),activation='sigmoid')])\n","classification_model1.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n","    loss=tf.keras.losses.BinaryCrossentropy(),\n","    metrics=['accuracy'])\n","history1=classification_model1.fit(df.drop(columns=['label']),df['label'],epochs=200,verbose=0,\n","                                 validation_split=0.2)\n","plt.plot(history1.history['loss'],label='loss')\n","plt.plot(history1.history['val_loss'],label='val_loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classification_model2=tf.keras.Sequential(\n","    [tf.keras.layers.Dense(1,input_shape=(2,),activation='relu')])\n","classification_model2.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n","    loss=tf.keras.losses.BinaryCrossentropy(),\n","    metrics=['accuracy'])\n","history2=classification_model2.fit(df.drop(columns=['label']),df['label'],epochs=200,verbose=0,\n","                                 validation_split=0.2)\n","plt.plot(history2.history['loss'],label='loss')\n","plt.plot(history2.history['val_loss'],label='val_loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["relu seems to be converging around 30 epochs while sigmoid continues to drop after 200 epochs with given learning rate(0.01)"]},{"cell_type":"markdown","metadata":{},"source":["bigger data without visualization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
